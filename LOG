This version is amazing

In 109th iteration i got 0.07 and 0.06 for train and validation loss with both accuracy above 90%
Though the file in interest is weights.108 shows . Not sure why weight file is 1 less may be because of counting from 0.

Similar result for 121 iteration.


Key modifications
1. Not loading pretrained weights. Training from scratch
2. Tried to put Learning rate and other SGD param close to or  as per original vgg16

Note that this does not have fc layers of vgg16 (It was removed by origninal author)



================================

Restarting training because the entire data got deleted on docker :(


===============================

16/12/2016

Error while running command
Exception: Error when checking model input: expected zeropadding2d_input_1 to have 4 dimensions, but got array with shape (0, 1)

Solution:
Fixed by adding pos and neg folders and giving the correct path --dataset_path
 python cam.py --dataset_path "../../trainsmall/" --train TRAIN

Error
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,256,56,56]
         [[Node: Conv2D_6 = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:1"](transpose_22, transpose_23)]]

Solution:
Switched to cpu mode for access to higher memory

-------rama + ../../trainsmall/pos ../../trainsmall/neg
Training..
Train on 1760 samples, validate on 440 samples
Epoch 1/200
1760/1760 [==============================] - 4839s - loss: 0.3273 - acc: 0.8949 - val_loss: 0.3064 - val_acc: 0.9068
Epoch 2/200
1760/1760 [==============================] - 4786s - loss: 0.2993 - acc: 0.9097 - val_loss: 0.2820 - val_acc: 0.9068
Epoch 3/200
1760/1760 [==============================] - 4834s - loss: 0.2703 - acc: 0.9097 - val_loss: 0.2426 - val_acc: 0.9068
Epoch 4/200
1760/1760 [==============================] - 4837s - loss: 0.2437 - acc: 0.9097 - val_loss: 0.2137 - val_acc: 0.9068
Epoch 5/200
1760/1760 [==============================] - 4838s - loss: 0.2629 - acc: 0.9097 - val_loss: 0.2222 - val_acc: 0.9068
Epoch 6/200
1760/1760 [==============================] - 4861s - loss: 0.2270 - acc: 0.9097 - val_loss: 0.2031 - val_acc: 0.9068
Epoch 7/200
1760/1760 [==============================] - 4526s - loss: 0.2981 - acc: 0.9097 - val_loss: 0.3001 - val_acc: 0.9068
Epoch 8/200
1760/1760 [==============================] - 4567s - loss: 0.2692 - acc: 0.9097 - val_loss: 0.2515 - val_acc: 0.9068
Epoch 9/200
1760/1760 [==============================] - 4525s - loss: 0.2584 - acc: 0.9097 - val_loss: 0.2643 - val_acc: 0.9068
Epoch 10/200
1760/1760 [==============================] - 4507s - loss: 0.2535 - acc: 0.9097 - val_loss: 0.2570 - val_acc: 0.9068
Epoch 11/200
1760/1760 [==============================] - 4584s - loss: 0.2480 - acc: 0.9097 - val_loss: 0.2446 - val_acc: 0.9068
Epoch 12/200
1760/1760 [==============================] - 4554s - loss: 0.2409 - acc: 0.9097 - val_loss: 0.2317 - val_acc: 0.9068
Epoch 13/200
1760/1760 [==============================] - 4488s - loss: 0.2377 - acc: 0.9097 - val_loss: 0.2248 - val_acc: 0.9068
Epoch 14/200
1760/1760 [==============================] - 4542s - loss: 0.2249 - acc: 0.9097 - val_loss: 0.2092 - val_acc: 0.9068
Epoch 15/200
1760/1760 [==============================] - 4574s - loss: 0.2266 - acc: 0.9097 - val_loss: 0.2171 - val_acc: 0.9068
Epoch 16/200
1760/1760 [==============================] - 4512s - loss: 0.2321 - acc: 0.9097 - val_loss: 0.3090 - val_acc: 0.9068
Epoch 17/200
  96/1760 [>.............................] - ETA: 3906s - loss: 0.1689 - acc: 0.9583

=========================================================== Got killed randomly. Do not know the reason

Running again by loading iteration 14 from the previous training. Not sure if the automatic validation split will take the
same files. But continuing as we have a different untouched training/validation data. Notice the slight improvement in all
parameters (suggests different validation set as these improvements are not seen in previous run if it continued.)

Training..
Train on 1760 samples, validate on 440 samples
Epoch 1/200
1760/1760 [==============================] - 2346s - loss: 0.2211 - acc: 0.9085 - val_loss: 0.2562 - val_acc: 0.9114
Epoch 2/200
1760/1760 [==============================] - 4708s - loss: 0.2363 - acc: 0.9085 - val_loss: 0.2524 - val_acc: 0.9114
Epoch 3/200
1760/1760 [==============================] - 4692s - loss: 0.2252 - acc: 0.9085 - val_loss: 0.2259 - val_acc: 0.9114
Epoch 4/200
1760/1760 [==============================] - 4701s - loss: 0.2202 - acc: 0.9085 - val_loss: 0.2232 - val_acc: 0.9114
Epoch 5/200
1760/1760 [==============================] - 4708s - loss: 0.2262 - acc: 0.9085 - val_loss: 0.2154 - val_acc: 0.9114
Epoch 6/200
1760/1760 [==============================] - 4695s - loss: 0.2137 - acc: 0.9085 - val_loss: 0.2320 - val_acc: 0.9114
Epoch 7/200
1760/1760 [==============================] - 4688s - loss: 0.2770 - acc: 0.9085 - val_loss: 0.2277 - val_acc: 0.9114
Epoch 8/200
1760/1760 [==============================] - 4706s - loss: 0.2089 - acc: 0.9085 - val_loss: 0.2233 - val_acc: 0.9114
Epoch 9/200
1760/1760 [==============================] - 4697s - loss: 0.2185 - acc: 0.9085 - val_loss: 0.2170 - val_acc: 0.9114
Epoch 10/200
1760/1760 [==============================] - 4698s - loss: 0.2073 - acc: 0.9085 - val_loss: 0.2143 - val_acc: 0.9114
Epoch 11/200
1760/1760 [==============================] - 4714s - loss: 0.2197 - acc: 0.9085 - val_loss: 0.2070 - val_acc: 0.9114
Epoch 12/200
1760/1760 [==============================] - 4734s - loss: 0.2095 - acc: 0.9085 - val_loss: 0.2224 - val_acc: 0.9114
Epoch 13/200
1760/1760 [==============================] - 4730s - loss: 0.2082 - acc: 0.9085 - val_loss: 0.2064 - val_acc: 0.9114
Epoch 14/200
1760/1760 [==============================] - 4753s - loss: 0.1972 - acc: 0.9085 - val_loss: 0.2160 - val_acc: 0.9114
Epoch 15/200
1760/1760 [==============================] - 4737s - loss: 0.1998 - acc: 0.9085 - val_loss: 0.2171 - val_acc: 0.9114
Epoch 16/200
1760/1760 [==============================] - 4753s - loss: 0.1975 - acc: 0.9097 - val_loss: 0.2029 - val_acc: 0.9114

===============] - 4737s - loss: 0.1998 - acc: 0.9085 - val_loss: 0.2171 - val_acc: 0.9114
Epoch 16/200
1760/1760 [==============================] - 4753s - loss: 0.1975 - acc: 0.9097 - val_loss: 0.2029 - val_acc: 0.9114
Epoch 17/200
1760/1760 [==============================] - 4763s - loss: 0.1884 - acc: 0.9085 - val_loss: 0.1940 - val_acc: 0.9114
Epoch 18/200
1760/1760 [==============================] - 4816s - loss: 0.1888 - acc: 0.9080 - val_loss: 0.1939 - val_acc: 0.9114
Epoch 19/200
1760/1760 [==============================] - 4800s - loss: 0.1860 - acc: 0.9057 - val_loss: 0.1903 - val_acc: 0.9114
Epoch 20/200
1760/1760 [==============================] - 4792s - loss: 0.2082 - acc: 0.9062 - val_loss: 0.2169 - val_acc: 0.9114
Epoch 21/200
1760/1760 [==============================] - 4774s - loss: 0.1969 - acc: 0.9085 - val_loss: 0.1903 - val_acc: 0.9114
Epoch 22/200
1760/1760 [==============================] - 4791s - loss: 0.1861 - acc: 0.9062 - val_loss: 0.2022 - val_acc: 0.9114
Epoch 23/200
1760/1760 [==============================] - 4790s - loss: 0.1883 - acc: 0.9057 - val_loss: 0.1920 - val_acc: 0.9114
Epoch 24/200
1760/1760 [==============================] - 4781s - loss: 0.1862 - acc: 0.9091 - val_loss: 0.1991 - val_acc: 0.9114
Epoch 25/200
1760/1760 [==============================] - 4804s - loss: 0.1873 - acc: 0.9074 - val_loss: 0.1942 - val_acc: 0.9114
Epoch 26/200
1760/1760 [==============================] - 4795s - loss: 0.1816 - acc: 0.9080 - val_loss: 0.2105 - val_acc: 0.9114
Epoch 27/200
1760/1760 [==============================] - 4810s - loss: 0.1814 - acc: 0.9074 - val_loss: 0.2424 - val_acc: 0.9114
Epoch 28/200
1760/1760 [==============================] - 4777s - loss: 0.1883 - acc: 0.9085 - val_loss: 0.1866 - val_acc: 0.9114
Epoch 29/200
1760/1760 [==============================] - 4827s - loss: 0.1850 - acc: 0.9085 - val_loss: 0.1972 - val_acc: 0.9091
Epoch 30/200
1760/1760 [==============================] - 4807s - loss: 0.1839 - acc: 0.9074 - val_loss: 0.1916 - val_acc: 0.9114
Epoch 31/200
1760/1760 [==============================] - 4816s - loss: 0.1780 - acc: 0.9085 - val_loss: 0.1852 - val_acc: 0.9114
Epoch 32/200
1760/1760 [==============================] - 4819s - loss: 0.1839 - acc: 0.9051 - val_loss: 0.1924 - val_acc: 0.9114
Epoch 33/200
1760/1760 [==============================] - 4857s - loss: 0.1815 - acc: 0.9085 - val_loss: 0.1888 - val_acc: 0.9159
Epoch 34/200
1760/1760 [==============================] - 4836s - loss: 0.1782 - acc: 0.9062 - val_loss: 0.1878 - val_acc: 0.9114
Epoch 35/200
1760/1760 [==============================] - 4872s - loss: 0.1767 - acc: 0.9068 - val_loss: 0.1867 - val_acc: 0.9114
Epoch 36/200
1760/1760 [==============================] - 4839s - loss: 0.1773 - acc: 0.9080 - val_loss: 0.1855 - val_acc: 0.9136
get_model()

Epoch 20/200
1760/1760 [==============================] - 4792s - loss: 0.2082 - acc: 0.9062 - val_loss: 0.2169 - val_acc: 0.9114
Epoch 21/200
1760/1760 [==============================] - 4774s - loss: 0.1969 - acc: 0.9085 - val_loss: 0.1903 - val_acc: 0.9114
Epoch 22/200
1760/1760 [==============================] - 4791s - loss: 0.1861 - acc: 0.9062 - val_loss: 0.2022 - val_acc: 0.9114
Epoch 23/200
1760/1760 [==============================] - 4790s - loss: 0.1883 - acc: 0.9057 - val_loss: 0.1920 - val_acc: 0.9114
Epoch 24/200
1760/1760 [==============================] - 4781s - loss: 0.1862 - acc: 0.9091 - val_loss: 0.1991 - val_acc: 0.9114
Epoch 25/200
1760/1760 [==============================] - 4804s - loss: 0.1873 - acc: 0.9074 - val_loss: 0.1942 - val_acc: 0.9114
Epoch 26/200
1760/1760 [==============================] - 4795s - loss: 0.1816 - acc: 0.9080 - val_loss: 0.2105 - val_acc: 0.9114
Epoch 27/200
1760/1760 [==============================] - 4810s - loss: 0.1814 - acc: 0.9074 - val_loss: 0.2424 - val_acc: 0.9114
Epoch 28/200
1760/1760 [==============================] - 4777s - loss: 0.1883 - acc: 0.9085 - val_loss: 0.1866 - val_acc: 0.9114
Epoch 29/200
1760/1760 [==============================] - 4827s - loss: 0.1850 - acc: 0.9085 - val_loss: 0.1972 - val_acc: 0.9091
Epoch 30/200
1760/1760 [==============================] - 4807s - loss: 0.1839 - acc: 0.9074 - val_loss: 0.1916 - val_acc: 0.9114
Epoch 31/200
1760/1760 [==============================] - 4816s - loss: 0.1780 - acc: 0.9085 - val_loss: 0.1852 - val_acc: 0.9114
Epoch 32/200
1760/1760 [==============================] - 4819s - loss: 0.1839 - acc: 0.9051 - val_loss: 0.1924 - val_acc: 0.9114
Epoch 33/200
1760/1760 [==============================] - 4857s - loss: 0.1815 - acc: 0.9085 - val_loss: 0.1888 - val_acc: 0.9159
Epoch 34/200
1760/1760 [==============================] - 4836s - loss: 0.1782 - acc: 0.9062 - val_loss: 0.1878 - val_acc: 0.9114
Epoch 35/200
1760/1760 [==============================] - 4872s - loss: 0.1767 - acc: 0.9068 - val_loss: 0.1867 - val_acc: 0.9114
Epoch 36/200
1760/1760 [==============================] - 4839s - loss: 0.1773 - acc: 0.9080 - val_loss: 0.1855 - val_acc: 0.9136
Epoch 37/200
1760/1760 [==============================] - 4858s - loss: 0.1790 - acc: 0.9080 - val_loss: 0.2012 - val_acc: 0.9205
Epoch 38/200
1760/1760 [==============================] - 4885s - loss: 0.1763 - acc: 0.9074 - val_loss: 0.1840 - val_acc: 0.9114
Epoch 39/200
1760/1760 [==============================] - 4892s - loss: 0.1763 - acc: 0.9085 - val_loss: 0.1950 - val_acc: 0.9114
Epoch 40/200
1760/1760 [==============================] - 4842s - loss: 0.1789 - acc: 0.9074 - val_loss: 0.1889 - val_acc: 0.9114
Epoch 41/200
1760/1760 [==============================] - 4870s - loss: 0.1765 - acc: 0.9085 - val_loss: 0.1877 - val_acc: 0.9136
Epoch 42/200
 640/1760 [=========>....................] - ETA: 2834s - loss: 0.1726 - acc: 0.9125

Killing it as i am not able to run quiver.


-------------------------------- How i made quiver to install successfully
git clone and then used the following comment from windows issue on github
It worked for me when I replaced 'quiverboard/dist/*' with ['quiverboard/dist/*'] in setup.py, compiled the client and then installed with python setup.py install.

was hit with this 500 internal server error when accessing /inputs url
File "/usr/local/lib/python2.7/dist-packages/flask/json.py", line 237, in jsonify
    return current_app.response_class(dumps(dict(*args, **kwargs),
ValueError: dictionary update sequence element #0 has length 28; 2 is required
most likely fix
upgrade flask
----

when looked at quiver visualizations, slightly disappointed as most of the images looked had just horizontal lines.
was expecting it to learn and show better features atlease in higher layers.
Also this is different from CAM visualization. Not sure if there is a quiver setup issue! (In CAM visualization we can see moles getting a bit more attention)
---
Starting with almost original settings to see if quiver shows better feature learnings
remember to change hardcoded quiver servier image size

Notice this has achieved 100% training accouracy and 0.0.. loss but high validation loss (I was expecting a lower than .9 validation accuracy though)
Also note that some iterations take only 300s while most of the initial ones take 2730s . Not sure why

Output
-------rama + ../../trainsmall/pos ../../trainsmall/neg
Training..
Train on 1760 samples, validate on 440 samples
Epoch 1/200
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 3.02GiB. The caller indicates that this is not a failure, but may mean that there c ould be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 3.02GiB. The caller indicates that this is not a failure, but may mean that there c
ould be performance gains if more memory is available.
 608/1760 [=========>....................] - ETA: 2730s - loss: 0.4625 - acc: 0.9013

1280/1760 [====================>.........] - ETA: 1002s - loss: 0.4283 - acc: 0.9000
1312/1760 [=====================>........] - ETA: 929s - loss: 0.4278 - acc: 0.8994
1632/1760 [==========================>...] - ETA: 262s - loss: 0.3904 - acc: 0.9075
1728/1760 [============================>.] - ETA: 65s - loss: 0.3848 - acc: 0.9091 W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 
1760/1760 [==============================] - 4096s - loss: 0.3844 - acc: 0.9091 - val_loss: 0.3220 - val_acc: 0.9068 memory is available.
Epoch 2/200
1376/1760 [======================>.......] - ETA: 723s - loss: 0.3241 - acc: 0.9084
1760/1760 [==============================] - 3597s - loss: 0.3286 - acc: 0.9097 - val_loss: 0.2939 - val_acc: 0.9068
Epoch 3/200

 576/1760 [========>.....................] - ETA: 2293s - loss: 0.3383 - acc: 0.9097
1760/1760 [==============================] - 3604s - loss: 0.3152 - acc: 0.9097 - val_loss: 0.4097 - val_acc: 0.9068
Epoch 4/200
1760/1760 [==============================] - 3695s - loss: 0.3265 - acc: 0.9097 - val_loss: 0.2793 - val_acc: 0.9068
Epoch 5/200
1760/1760 [==============================] - 3717s - loss: 0.2579 - acc: 0.9097 - val_loss: 0.2518 - val_acc: 0.9068
Epoch 6/200
1760/1760 [==============================] - 3792s - loss: 0.2269 - acc: 0.9097 - val_loss: 0.2123 - val_acc: 0.9068
Epoch 7/200
1760/1760 [==============================] - 3690s - loss: 0.2044 - acc: 0.9091 - val_loss: 0.2149 - val_acc: 0.9068
Epoch 8/200
1760/1760 [==============================] - 3624s - loss: 0.2005 - acc: 0.9097 - val_loss: 0.2067 - val_acc: 0.9068
Epoch 9/200
1760/1760 [==============================] - 3555s - loss: 0.2054 - acc: 0.9040 - val_loss: 0.2046 - val_acc: 0.9068
Epoch 10/200
1760/1760 [==============================] - 3613s - loss: 0.1967 - acc: 0.9097 - val_loss: 0.2097 - val_acc: 0.9068
Epoch 11/200
1760/1760 [==============================] - 3714s - loss: 0.1883 - acc: 0.9097 - val_loss: 0.2363 - val_acc: 0.9068
Epoch 12/200
1760/1760 [==============================] - 3615s - loss: 0.1825 - acc: 0.9097 - val_loss: 0.2206 - val_acc: 0.9068
Epoch 13/200
1760/1760 [==============================] - 3670s - loss: 0.1840 - acc: 0.9097 - val_loss: 0.1922 - val_acc: 0.9068
Epoch 14/200
1760/1760 [==============================] - 3579s - loss: 0.1845 - acc: 0.9097 - val_loss: 0.2068 - val_acc: 0.9068
Epoch 15/200
1760/1760 [==============================] - 3451s - loss: 0.1839 - acc: 0.9097 - val_loss: 0.1923 - val_acc: 0.9068
Epoch 16/200
1760/1760 [==============================] - 3637s - loss: 0.1784 - acc: 0.9097 - val_loss: 0.1932 - val_acc: 0.9068
Epoch 17/200
1760/1760 [==============================] - 3588s - loss: 0.1782 - acc: 0.9097 - val_loss: 0.1835 - val_acc: 0.9068
Epoch 18/200
1760/1760 [==============================] - 3686s - loss: 0.1831 - acc: 0.9097 - val_loss: 0.1853 - val_acc: 0.9068
Epoch 19/200
1760/1760 [==============================] - 3650s - loss: 0.1806 - acc: 0.9097 - val_loss: 0.1885 - val_acc: 0.9068
Epoch 20/200
1760/1760 [==============================] - 3702s - loss: 0.1779 - acc: 0.9097 - val_loss: 0.1912 - val_acc: 0.9068
Epoch 21/200
1760/1760 [==============================] - 3686s - loss: 0.1781 - acc: 0.9097 - val_loss: 0.1876 - val_acc: 0.9068
Epoch 22/200
1760/1760 [==============================] - 3598s - loss: 0.1699 - acc: 0.9102 - val_loss: 0.1793 - val_acc: 0.9068
Epoch 23/200
1760/1760 [==============================] - 3533s - loss: 0.1769 - acc: 0.9085 - val_loss: 0.1810 - val_acc: 0.9068
Epoch 24/200
1760/1760 [==============================] - 3699s - loss: 0.1721 - acc: 0.9097 - val_loss: 0.1869 - val_acc: 0.9068
Epoch 25/200
1760/1760 [==============================] - 3608s - loss: 0.1747 - acc: 0.9097 - val_loss: 0.1860 - val_acc: 0.9068
Epoch 26/200
1760/1760 [==============================] - 3577s - loss: 0.1730 - acc: 0.9091 - val_loss: 0.1959 - val_acc: 0.9068
Epoch 27/200
1760/1760 [==============================] - 3653s - loss: 0.1743 - acc: 0.9091 - val_loss: 0.1917 - val_acc: 0.9068
Epoch 28/200
1760/1760 [==============================] - 3586s - loss: 0.1719 - acc: 0.9097 - val_loss: 0.1803 - val_acc: 0.9068
Epoch 29/200
1760/1760 [==============================] - 3523s - loss: 0.1722 - acc: 0.9091 - val_loss: 0.1876 - val_acc: 0.9068
Epoch 30/200
1760/1760 [==============================] - 3417s - loss: 0.1744 - acc: 0.9097 - val_loss: 0.1827 - val_acc: 0.9068
Epoch 31/200
1760/1760 [==============================] - 3639s - loss: 0.1686 - acc: 0.9097 - val_loss: 0.1843 - val_acc: 0.9068
Epoch 32/200
1760/1760 [==============================] - 3586s - loss: 0.1693 - acc: 0.9091 - val_loss: 0.2420 - val_acc: 0.9068
Epoch 33/200
1760/1760 [==============================] - 3597s - loss: 0.1696 - acc: 0.9085 - val_loss: 0.1806 - val_acc: 0.9068
Epoch 34/200
1760/1760 [==============================] - 3619s - loss: 0.1689 - acc: 0.9102 - val_loss: 0.1803 - val_acc: 0.9068
Epoch 35/200
1760/1760 [==============================] - 3639s - loss: 0.1688 - acc: 0.9091 - val_loss: 0.1744 - val_acc: 0.9068
Epoch 36/200
1760/1760 [==============================] - 3672s - loss: 0.1688 - acc: 0.9097 - val_loss: 0.1780 - val_acc: 0.9068
Epoch 37/200
1760/1760 [==============================] - 3548s - loss: 0.1695 - acc: 0.9097 - val_loss: 0.1876 - val_acc: 0.9045
Epoch 38/200
1760/1760 [==============================] - 3570s - loss: 0.1761 - acc: 0.9091 - val_loss: 0.1789 - val_acc: 0.9068
Epoch 39/200
1760/1760 [==============================] - 3677s - loss: 0.1727 - acc: 0.9108 - val_loss: 0.1812 - val_acc: 0.9068
Epoch 40/200
1760/1760 [==============================] - 3637s - loss: 0.1674 - acc: 0.9108 - val_loss: 0.1739 - val_acc: 0.9068
Epoch 41/200
1760/1760 [==============================] - 3561s - loss: 0.1672 - acc: 0.9080 - val_loss: 0.1762 - val_acc: 0.9045
Epoch 42/200
1760/1760 [==============================] - 3549s - loss: 0.1623 - acc: 0.9108 - val_loss: 0.1799 - val_acc: 0.9068
Epoch 43/200
1760/1760 [==============================] - 3657s - loss: 0.1628 - acc: 0.9114 - val_loss: 0.1752 - val_acc: 0.9068
Epoch 44/200
1760/1760 [==============================] - 3521s - loss: 0.1626 - acc: 0.9148 - val_loss: 0.1766 - val_acc: 0.8955
Epoch 45/200
1760/1760 [==============================] - 3655s - loss: 0.1601 - acc: 0.9131 - val_loss: 0.1744 - val_acc: 0.9000
Epoch 46/200
1760/1760 [==============================] - 3730s - loss: 0.1591 - acc: 0.9114 - val_loss: 0.1714 - val_acc: 0.9000
Epoch 47/200
1760/1760 [==============================] - 3712s - loss: 0.1627 - acc: 0.9102 - val_loss: 0.1870 - val_acc: 0.9068
Epoch 48/200
1760/1760 [==============================] - 3584s - loss: 0.1605 - acc: 0.9182 - val_loss: 0.1821 - val_acc: 0.9045
Epoch 49/200
1760/1760 [==============================] - 3611s - loss: 0.1595 - acc: 0.9148 - val_loss: 0.1756 - val_acc: 0.9114
Epoch 50/200
1760/1760 [==============================] - 3781s - loss: 0.1543 - acc: 0.9210 - val_loss: 0.1942 - val_acc: 0.9023
Epoch 51/200
1760/1760 [==============================] - 3586s - loss: 0.1585 - acc: 0.9165 - val_loss: 0.1697 - val_acc: 0.9091
Epoch 52/200
1760/1760 [==============================] - 3679s - loss: 0.1521 - acc: 0.9193 - val_loss: 0.1803 - val_acc: 0.9068
Epoch 53/200
1760/1760 [==============================] - 3615s - loss: 0.1528 - acc: 0.9222 - val_loss: 0.1759 - val_acc: 0.9136
Epoch 54/200
1760/1760 [==============================] - 2260s - loss: 0.1551 - acc: 0.9239 - val_loss: 0.1776 - val_acc: 0.9205
Epoch 55/200
1760/1760 [==============================] - 112s - loss: 0.1547 - acc: 0.9216 - val_loss: 0.1751 - val_acc: 0.9045
poch 56/200
1760/1760 [==============================] - 113s - loss: 0.1520 - acc: 0.9278 - val_loss: 0.1852 - val_acc: 0.9023
Epoch 57/200
1760/1760 [==============================] - 113s - loss: 0.1469 - acc: 0.9278 - val_loss: 0.1789 - val_acc: 0.9068
Epoch 58/200
1760/1760 [==============================] - 113s - loss: 0.1465 - acc: 0.9250 - val_loss: 0.1770 - val_acc: 0.9091
Epoch 59/200
1760/1760 [==============================] - 113s - loss: 0.1524 - acc: 0.9250 - val_loss: 0.1768 - val_acc: 0.9136
Epoch 60/200
1760/1760 [==============================] - 113s - loss: 0.1453 - acc: 0.9358 - val_loss: 0.1934 - val_acc: 0.9114
Epoch 61/200
1760/1760 [==============================] - 113s - loss: 0.1468 - acc: 0.9273 - val_loss: 0.1879 - val_acc: 0.9091
Epoch 62/200
1760/1760 [==============================] - 113s - loss: 0.1464 - acc: 0.9301 - val_loss: 0.1845 - val_acc: 0.9023
Epoch 63/200
1760/1760 [==============================] - 113s - loss: 0.1463 - acc: 0.9256 - val_loss: 0.1870 - val_acc: 0.9068
Epoch 64/200
1760/1760 [==============================] - 113s - loss: 0.1461 - acc: 0.9256 - val_loss: 0.1772 - val_acc: 0.9023
Epoch 65/200
1760/1760 [==============================] - 113s - loss: 0.1426 - acc: 0.9313 - val_loss: 0.1750 - val_acc: 0.9068
Epoch 66/200
1760/1760 [==============================] - 113s - loss: 0.1405 - acc: 0.9324 - val_loss: 0.1889 - val_acc: 0.9023
Epoch 67/200
1760/1760 [==============================] - 210s - loss: 0.1556 - acc: 0.9256 - val_loss: 0.1876 - val_acc: 0.9068
Epoch 68/200
1760/1760 [==============================] - 113s - loss: 0.1453 - acc: 0.9284 - val_loss: 0.1843 - val_acc: 0.9182
Epoch 69/200
1760/1760 [==============================] - 113s - loss: 0.1369 - acc: 0.9341 - val_loss: 0.1827 - val_acc: 0.9205
Epoch 70/200
1760/1760 [==============================] - 113s - loss: 0.1394 - acc: 0.9324 - val_loss: 0.1977 - val_acc: 0.9045
Epoch 71/200
1760/1760 [==============================] - 113s - loss: 0.1350 - acc: 0.9341 - val_loss: 0.1805 - val_acc: 0.9136
Epoch 72/200
1760/1760 [==============================] - 113s - loss: 0.1462 - acc: 0.9244 - val_loss: 0.1727 - val_acc: 0.9136
Epoch 73/200
1760/1760 [==============================] - 113s - loss: 0.1287 - acc: 0.9381 - val_loss: 0.2420 - val_acc: 0.8682
Epoch 74/200
1760/1760 [==============================] - 113s - loss: 0.1342 - acc: 0.9381 - val_loss: 0.1942 - val_acc: 0.9114
Epoch 75/200
1760/1760 [==============================] - 113s - loss: 0.1269 - acc: 0.9432 - val_loss: 0.2426 - val_acc: 0.9000
Epoch 76/200
1760/1760 [==============================] - 113s - loss: 0.1239 - acc: 0.9420 - val_loss: 0.2208 - val_acc: 0.9136
Epoch 78/200
1760/1760 [==============================] - 113s - loss: 0.1420 - acc: 0.9273 - val_loss: 0.1876 - val_acc: 0.9136
Epoch 79/200
1760/1760 [==============================] - 113s - loss: 0.1246 - acc: 0.9369 - val_loss: 0.2026 - val_acc: 0.8886
Epoch 80/200
1760/1760 [==============================] - 113s - loss: 0.1230 - acc: 0.9477 - val_loss: 0.2305 - val_acc: 0.9091
Epoch 81/200
1760/1760 [==============================] - 113s - loss: 0.1211 - acc: 0.9420 - val_loss: 0.2359 - val_acc: 0.9068
Epoch 82/200
 640/1760 [=========>....................] - ETA: 67s - loss: 0.1394 - acc: 0.9328^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^1728/1760 [============================>.] - ETA: 1s - loss: 0.1299 - acc: 0.9404 ^[[B^[[B^[[BB^[[B^[[B^[[B
1760/1760 [==============================] - 113s - loss: 0.1293 - acc: 0.9409 - val_loss: 0.2288 - val_acc: 0.9023
Epoch 83/200
1760/1760 [==============================] - 113s - loss: 0.1147 - acc: 0.9511 - val_loss: 0.2378 - val_acc: 0.8977
Epoch 84/200
1760/1760 [==============================] - 113s - loss: 0.2068 - acc: 0.9364 - val_loss: 0.3126 - val_acc: 0.9068
Epoch 85/200
1760/1760 [==============================] - 113s - loss: 0.3286 - acc: 0.9097 - val_loss: 0.3490 - val_acc: 0.9068
Epoch 86/200
1760/1760 [==============================] - 113s - loss: 0.3338 - acc: 0.9097 - val_loss: 0.2870 - val_acc: 0.9068
Epoch 87/200
1760/1760 [==============================] - 113s - loss: 0.3394 - acc: 0.9097 - val_loss: 0.3295 - val_acc: 0.9068
Epoch 88/200
1760/1760 [==============================] - 113s - loss: 0.3056 - acc: 0.9097 - val_loss: 0.3235 - val_acc: 0.9068
Epoch 89/200
1760/1760 [==============================] - 113s - loss: 0.2707 - acc: 0.9097 - val_loss: 0.2552 - val_acc: 0.9068
Epoch 90/200
1760/1760 [==============================] - 113s - loss: 0.2317 - acc: 0.9097 - val_loss: 0.2157 - val_acc: 0.9068
Epoch 91/200
1760/1760 [==============================] - 113s - loss: 0.2059 - acc: 0.9097 - val_loss: 0.1995 - val_acc: 0.9068
Epoch 92/200
1760/1760 [==============================] - 113s - loss: 0.1855 - acc: 0.9097 - val_loss: 0.1938 - val_acc: 0.9068
Epoch 93/200
1760/1760 [==============================] - 113s - loss: 0.1808 - acc: 0.9097 - val_loss: 0.1909 - val_acc: 0.9068
Epoch 94/200
1760/1760 [==============================] - 113s - loss: 0.1824 - acc: 0.9097 - val_loss: 0.1921 - val_acc: 0.9068
Epoch 95/200
1760/1760 [==============================] - 113s - loss: 0.1772 - acc: 0.9097 - val_loss: 0.2024 - val_acc: 0.9068
Epoch 96/200
1760/1760 [==============================] - 113s - loss: 0.1761 - acc: 0.9097 - val_loss: 0.1845 - val_acc: 0.9068
Epoch 97/200
1760/1760 [==============================] - 113s - loss: 0.1715 - acc: 0.9102 - val_loss: 0.1886 - val_acc: 0.9068
Epoch 98/200
1760/1760 [==============================] - 113s - loss: 0.1669 - acc: 0.9091 - val_loss: 0.1768 - val_acc: 0.9068
Epoch 99/200
1760/1760 [==============================] - 113s - loss: 0.1706 - acc: 0.9108 - val_loss: 0.1848 - val_acc: 0.9068
Epoch 100/200
1760/1760 [==============================] - 113s - loss: 0.1654 - acc: 0.9114 - val_loss: 0.1774 - val_acc: 0.9068
Epoch 101/200
1760/1760 [==============================] - 113s - loss: 0.1610 - acc: 0.9176 - val_loss: 0.1860 - val_acc: 0.9068
Epoch 102/200
1760/1760 [==============================] - 113s - loss: 0.1591 - acc: 0.9193 - val_loss: 0.1868 - val_acc: 0.9091
Epoch 103/200
1760/1760 [==============================] - 113s - loss: 0.1600 - acc: 0.9170 - val_loss: 0.1804 - val_acc: 0.9091
Epoch 104/200
1760/1760 [==============================] - 113s - loss: 0.1647 - acc: 0.9131 - val_loss: 0.1758 - val_acc: 0.9068
Epoch 105/200
1760/1760 [==============================] - 113s - loss: 0.1586 - acc: 0.9159 - val_loss: 0.1790 - val_acc: 0.9068
Epoch 106/200
1760/1760 [==============================] - 113s - loss: 0.1514 - acc: 0.9256 - val_loss: 0.1948 - val_acc: 0.9068
Epoch 105/200
1760/1760 [==============================] - 113s - loss: 0.1586 - acc: 0.9159 - val_loss: 0.1790 - val_acc: 0.9068
Epoch 106/200
1760/1760 [==============================] - 113s - loss: 0.1514 - acc: 0.9256 - val_loss: 0.1948 - val_acc: 0.9068
Epoch 107/200
1760/1760 [==============================] - 113s - loss: 0.1504 - acc: 0.9256 - val_loss: 0.2053 - val_acc: 0.8909
Epoch 108/200
1760/1760 [==============================] - 113s - loss: 0.1591 - acc: 0.9159 - val_loss: 0.1815 - val_acc: 0.9114
Epoch 109/200
1760/1760 [==============================] - 113s - loss: 0.1461 - acc: 0.9250 - val_loss: 0.1836 - val_acc: 0.9114
Epoch 110/200
1760/1760 [==============================] - 113s - loss: 0.1453 - acc: 0.9318 - val_loss: 0.1848 - val_acc: 0.9205
Epoch 111/200
1760/1760 [==============================] - 113s - loss: 0.1481 - acc: 0.9324 - val_loss: 0.1972 - val_acc: 0.8886
Epoch 112/200
1760/1760 [==============================] - 113s - loss: 0.1457 - acc: 0.9273 - val_loss: 0.1736 - val_acc: 0.9159
Epoch 113/200
1760/1760 [==============================] - 113s - loss: 0.1499 - acc: 0.9216 - val_loss: 0.1686 - val_acc: 0.9159
Epoch 114/200
1760/1760 [==============================] - 113s - loss: 0.1427 - acc: 0.9278 - val_loss: 0.1962 - val_acc: 0.8909
Epoch 115/200
1760/1760 [==============================] - 2303s - loss: 0.1388 - acc: 0.9301 - val_loss: 0.1919 - val_acc: 0.9114
Epoch 116/200
1760/1760 [==============================] - 3198s - loss: 0.1390 - acc: 0.9318 - val_loss: 0.1801 - val_acc: 0.9159
Epoch 117/200
1760/1760 [==============================] - 3230s - loss: 0.1357 - acc: 0.9347 - val_loss: 0.2053 - val_acc: 0.8932
Epoch 118/200
1760/1760 [==============================] - 3339s - loss: 0.1348 - acc: 0.9398 - val_loss: 0.1891 - val_acc: 0.9091
Epoch 119/200
1760/1760 [==============================] - 3239s - loss: 0.1327 - acc: 0.9307 - val_loss: 0.1904 - val_acc: 0.9045
Epoch 120/200
1760/1760 [==============================] - 3316s - loss: 0.1335 - acc: 0.9335 - val_loss: 0.1872 - val_acc: 0.9114
Epoch 121/200
1760/1760 [==============================] - 3271s - loss: 0.1336 - acc: 0.9369 - val_loss: 0.1946 - val_acc: 0.9068
Epoch 122/200
1760/1760 [==============================] - 3314s - loss: 0.1325 - acc: 0.9335 - val_loss: 0.1845 - val_acc: 0.9000
Epoch 123/200
1760/1760 [==============================] - 3350s - loss: 0.1208 - acc: 0.9477 - val_loss: 0.2416 - val_acc: 0.8818
Epoch 124/200
1760/1760 [==============================] - 3244s - loss: 0.1320 - acc: 0.9398 - val_loss: 0.1937 - val_acc: 0.8955
Epoch 125/200
1760/1760 [==============================] - 3224s - loss: 0.1223 - acc: 0.9398 - val_loss: 0.2338 - val_acc: 0.9068
Epoch 126/200
1760/1760 [==============================] - 3312s - loss: 0.1223 - acc: 0.9415 - val_loss: 0.2575 - val_acc: 0.8773
Epoch 127/200
1760/1760 [==============================] - 3380s - loss: 0.1236 - acc: 0.9409 - val_loss: 0.1926 - val_acc: 0.9091
Epoch 128/200
1760/1760 [==============================] - 3341s - loss: 0.1231 - acc: 0.9398 - val_loss: 0.2086 - val_acc: 0.9045
Epoch 129/200
1760/1760 [==============================] - 3214s - loss: 0.1212 - acc: 0.9460 - val_loss: 0.1935 - val_acc: 0.9114
Epoch 130/200
1760/1760 [==============================] - 3282s - loss: 0.1135 - acc: 0.9449 - val_loss: 0.2427 - val_acc: 0.9000
Epoch 131/200
1760/1760 [==============================] - 3291s - loss: 0.1165 - acc: 0.9455 - val_loss: 0.2005 - val_acc: 0.9045
Epoch 132/200
1760/1760 [==============================] - 3167s - loss: 0.1172 - acc: 0.9477 - val_loss: 0.2050 - val_acc: 0.9114
Epoch 133/200
1760/1760 [==============================] - 3215s - loss: 0.1180 - acc: 0.9443 - val_loss: 0.1985 - val_acc: 0.8977
Epoch 134/200
1760/1760 [==============================] - 3230s - loss: 0.1115 - acc: 0.9500 - val_loss: 0.2134 - val_acc: 0.9114
Epoch 135/200
1760/1760 [==============================] - 3232s - loss: 0.1113 - acc: 0.9511 - val_loss: 0.2100 - val_acc: 0.9045
Epoch 136/200
1760/1760 [==============================] - 3309s - loss: 0.1081 - acc: 0.9534 - val_loss: 0.2018 - val_acc: 0.9023
Epoch 137/200
1760/1760 [==============================] - 3230s - loss: 0.1290 - acc: 0.9472 - val_loss: 0.1966 - val_acc: 0.9000
Epoch 138/200
1760/1760 [==============================] - 3244s - loss: 0.1161 - acc: 0.9466 - val_loss: 0.2412 - val_acc: 0.9136
Epoch 139/200
1760/1760 [==============================] - 3206s - loss: 0.1067 - acc: 0.9534 - val_loss: 0.1942 - val_acc: 0.9000
Epoch 140/200
1760/1760 [==============================] - 3262s - loss: 0.0935 - acc: 0.9585 - val_loss: 0.2260 - val_acc: 0.9023
Epoch 141/200
1760/1760 [==============================] - 3174s - loss: 0.0994 - acc: 0.9545 - val_loss: 0.2331 - val_acc: 0.9045
Epoch 142/200
1760/1760 [==============================] - 3224s - loss: 0.1019 - acc: 0.9591 - val_loss: 0.2028 - val_acc: 0.9114
Epoch 143/200
1760/1760 [==============================] - 3151s - loss: 0.0866 - acc: 0.9619 - val_loss: 0.2983 - val_acc: 0.9000
Epoch 144/200
1760/1760 [==============================] - 3137s - loss: 0.1009 - acc: 0.9580 - val_loss: 0.2350 - val_acc: 0.9023
Epoch 145/200
1760/1760 [==============================] - 2513s - loss: 0.0896 - acc: 0.9642 - val_loss: 0.2504 - val_acc: 0.9068
Epoch 146/200
1760/1760 [==============================] - 113s - loss: 0.0924 - acc: 0.9619 - val_loss: 0.2426 - val_acc: 0.9000
Epoch 147/200
1760/1760 [==============================] - 113s - loss: 0.0818 - acc: 0.9648 - val_loss: 0.2588 - val_acc: 0.9205
Epoch 148/200
1760/1760 [==============================] - 113s - loss: 0.1004 - acc: 0.9625 - val_loss: 0.1965 - val_acc: 0.9159
Epoch 149/200
1760/1760 [==============================] - 113s - loss: 0.0743 - acc: 0.9682 - val_loss: 0.3190 - val_acc: 0.9091
Epoch 150/200
1760/1760 [==============================] - 113s - loss: 0.0959 - acc: 0.9557 - val_loss: 0.3535 - val_acc: 0.9091
Epoch 151/200
1760/1760 [==============================] - 113s - loss: 0.0640 - acc: 0.9739 - val_loss: 0.3173 - val_acc: 0.9023
Epoch 152/200
1760/1760 [==============================] - 113s - loss: 0.0937 - acc: 0.9631 - val_loss: 0.2393 - val_acc: 0.8955
Epoch 153/200
1760/1760 [==============================] - 113s - loss: 0.0720 - acc: 0.9744 - val_loss: 0.3472 - val_acc: 0.9091
Epoch 154/200
1760/1760 [==============================] - 113s - loss: 0.0714 - acc: 0.9716 - val_loss: 0.1857 - val_acc: 0.9114
Epoch 155/200
1760/1760 [==============================] - 113s - loss: 0.0506 - acc: 0.9807 - val_loss: 0.3315 - val_acc: 0.906
Epoch 156/200
1760/1760 [==============================] - 113s - loss: 0.0514 - acc: 0.9807 - val_loss: 0.3351 - val_acc: 0.9045
Epoch 157/200
1760/1760 [==============================] - 113s - loss: 0.0924 - acc: 0.9727 - val_loss: 0.2685 - val_acc: 0.9091
Epoch 158/200
1760/1760 [==============================] - 113s - loss: 0.0611 - acc: 0.9773 - val_loss: 0.3018 - val_acc: 0.9023
Epoch 159/200
1760/1760 [==============================] - 113s - loss: 0.0317 - acc: 0.9864 - val_loss: 0.4680 - val_acc: 0.9023
Epoch 160/200
1760/1760 [==============================] - 113s - loss: 0.0642 - acc: 0.9767 - val_loss: 0.3127 - val_acc: 0.8977
Epoch 161/200
1760/1760 [==============================] - 113s - loss: 0.0839 - acc: 0.9670 - val_loss: 0.2291 - val_acc: 0.9068
Epoch 162/200
1760/1760 [==============================] - 113s - loss: 0.0377 - acc: 0.9869 - val_loss: 0.3766 - val_acc: 0.9000
Epoch 163/200
1760/1760 [==============================] - 113s - loss: 0.0263 - acc: 0.9898 - val_loss: 0.4330 - val_acc: 0.9045
Epoch 164/200
1760/1760 [==============================] - 113s - loss: 0.0730 - acc: 0.9693 - val_loss: 0.3375 - val_acc: 0.9159
Epoch 165/200
1760/1760 [==============================] - 113s - loss: 0.0571 - acc: 0.9795 - val_loss: 0.2497 - val_acc: 0.9068
Epoch 166/200
1760/1760 [==============================] - 113s - loss: 0.0390 - acc: 0.9841 - val_loss: 0.3661 - val_acc: 0.9068
Epoch 167/200
1760/1760 [==============================] - 113s - loss: 0.0183 - acc: 0.9926 - val_loss: 0.3635 - val_acc: 0.9091
Epoch 168/200
1760/1760 [==============================] - 113s - loss: 0.0258 - acc: 0.9898 - val_loss: 0.4714 - val_acc: 0.9045
Epoch 169/200
1760/1760 [==============================] - 113s - loss: 0.0411 - acc: 0.9864 - val_loss: 0.3855 - val_acc: 0.9000
Epoch 170/200
1760/1760 [==============================] - 113s - loss: 0.0106 - acc: 0.9977 - val_loss: 0.3752 - val_acc: 0.8977
Epoch 171/200
1760/1760 [==============================] - 113s - loss: 0.0191 - acc: 0.9960 - val_loss: 0.5270 - val_acc: 0.9023
Epoch 172/200
1760/1760 [==============================] - 113s - loss: 0.0025 - acc: 0.9989 - val_loss: 0.5623 - val_acc: 0.9023
Epoch 173/200
1760/1760 [==============================] - 113s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.6544 - val_acc: 0.9000
Epoch 174/200
1760/1760 [==============================] - 113s - loss: 3.9172e-04 - acc: 1.0000 - val_loss: 0.6977 - val_acc: 0.8977
Epoch 175/200
1760/1760 [==============================] - 113s - loss: 2.6411e-04 - acc: 1.0000 - val_loss: 0.7265 - val_acc: 0.8955
Epoch 176/200
1760/1760 [==============================] - 113s - loss: 2.0256e-04 - acc: 1.0000 - val_loss: 0.7429 - val_acc: 0.8955
Epoch 177/200
1760/1760 [==============================] - 113s - loss: 1.6243e-04 - acc: 1.0000 - val_loss: 0.7668 - val_acc: 0.8955
Epoch 178/200
1760/1760 [==============================] - 113s - loss: 1.3874e-04 - acc: 1.0000 - val_loss: 0.7767 - val_acc: 0.8955
Epoch 179/200
1760/1760 [==============================] - 113s - loss: 1.2019e-04 - acc: 1.0000 - val_loss: 0.7896 - val_acc: 0.8955
Epoch 180/200
1760/1760 [==============================] - 113s - loss: 1.0442e-04 - acc: 1.0000 - val_loss: 0.8029 - val_acc: 0.8955
Epoch 181/200
1760/1760 [==============================] - 113s - loss: 9.2598e-05 - acc: 1.0000 - val_loss: 0.8108 - val_acc: 0.8955
Epoch 182/200
1760/1760 [==============================] - 113s - loss: 8.3538e-05 - acc: 1.0000 - val_loss: 0.8193 - val_acc: 0.8955
Epoch 183/200
1760/1760 [==============================] - 113s - loss: 7.5792e-05 - acc: 1.0000 - val_loss: 0.8268 - val_acc: 0.8955
Epoch 184/200
1760/1760 [==============================] - 113s - loss: 6.9450e-05 - acc: 1.0000 - val_loss: 0.8369 - val_acc: 0.8955
Epoch 185/200
1760/1760 [==============================] - 113s - loss: 6.3933e-05 - acc: 1.0000 - val_loss: 0.8424 - val_acc: 0.8955
Epoch 186/200
1760/1760 [==============================] - 113s - loss: 5.9030e-05 - acc: 1.0000 - val_loss: 0.8482 - val_acc: 0.8955
Epoch 187/200
1760/1760 [==============================] - 113s - loss: 5.5226e-05 - acc: 1.0000 - val_loss: 0.8552 - val_acc: 0.8955
Epoch 188/200
1760/1760 [==============================] - 113s - loss: 5.1210e-05 - acc: 1.0000 - val_loss: 0.8625 - val_acc: 0.8955
Epoch 189/200
1760/1760 [==============================] - 113s - loss: 4.7982e-05 - acc: 1.0000 - val_loss: 0.8675 - val_acc: 0.8955
Epoch 190/200
1760/1760 [==============================] - 113s - loss: 4.5383e-05 - acc: 1.0000 - val_loss: 0.8727 - val_acc: 0.8955
Epoch 191/200
1760/1760 [==============================] - 113s - loss: 4.2848e-05 - acc: 1.0000 - val_loss: 0.8777 - val_acc: 0.8955
Epoch 192/200
1760/1760 [==============================] - 113s - loss: 4.0398e-05 - acc: 1.0000 - val_loss: 0.8836 - val_acc: 0.8955
Epoch 193/200
1760/1760 [==============================] - 113s - loss: 3.8431e-05 - acc: 1.0000 - val_loss: 0.8874 - val_acc: 0.8955
Epoch 194/200
1760/1760 [==============================] - 113s - loss: 3.6572e-05 - acc: 1.0000 - val_loss: 0.8919 - val_acc: 0.8955
Epoch 195/200
1760/1760 [==============================] - 113s - loss: 3.4750e-05 - acc: 1.0000 - val_loss: 0.8964 - val_acc: 0.8955
Epoch 196/200
1760/1760 [==============================] - 113s - loss: 3.3043e-05 - acc: 1.0000 - val_loss: 0.9002 - val_acc: 0.8955
Epoch 197/200
1760/1760 [==============================] - 567s - loss: 3.1796e-05 - acc: 1.0000 - val_loss: 0.9042 - val_acc: 0.8955
Epoch 198/200
1760/1760 [==============================] - 113s - loss: 3.0426e-05 - acc: 1.0000 - val_loss: 0.9076 - val_acc: 0.8955
Epoch 199/200
1760/1760 [==============================] - 113s - loss: 2.9209e-05 - acc: 1.0000 - val_loss: 0.9108 - val_acc: 0.8955
Epoch 200/200
1760/1760 [==============================] - 113s - loss: 2.8021e-05 - acc: 1.0000 - val_loss: 0.9138 - val_acc: 0.8932



Evaluation
root@f5e57551b879:/datamnt/github/keras-cam-melanoma# python cam.py --image_path ../../train/malignant/558d6362bae47801cf734d17.jpg --model_path archiveweights-28dec/weights.53-loss0.155-acc0.924-valloss0.178-valacc0.920.hdf5 

predictions for image  ../../train/malignant/558d6362bae47801cf734d17.jpg  is  [[ 0.72275358  0.27724642]]

root@f5e57551b879:/datamnt/github/keras-cam-melanoma# python cam.py --image_path ../../train/malignant/558d6362bae47801cf734d17.jpg --model_path archiveweights-28dec/weights.136-loss0.129-acc0.947-valloss0.197-valacc0.900.hdf5 
predictions for image  ../../train/malignant/558d6362bae47801cf734d17.jpg  is  [[ 0.62246692  0.37753308]]
CAM shows better and more activation regions in 2nd case (iteration 136)

Higher iteration weights which have very good training accuracy but bad validation loss (.25) shows CAM highlighted regions near borders.
================================================

Adam lr = 0.001
1760/1760 [==============================] - 137s - loss: 1.4395 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 2/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 3/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 4/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 5/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 6/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 7/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 8/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 9/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 10/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 11/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 12/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 13/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 14/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 15/200
1760/1760 [==============================] - 109s - loss: 1.4470 - acc: 0.9102 - val_loss: 1.5385 - val_acc: 0.9045
Epoch 16/200

=======================================================

wow Adam with lr 0.3 converged in a single run
1760/1760 [==============================] - 137s - loss: 0.1466 - acc: 0.9080 - val_loss: 1.1921e-07 - val_acc: 0.9136
Epoch 2/200
1760/1760 [==============================] - 109s - loss: 1.1921e-07 - acc: 0.9080 - val_loss: 1.1921e-07 - val_acc: 0.9136
Epoch 3/200
1760/1760 [==============================] - 109s - loss: 1.1921e-07 - acc: 0.9080 - val_loss: 1.1921e-07 - val_acc: 0.9136
Epoch 4/200
1760/1760 [==============================] - 109s - loss: 1.1921e-07 - acc: 0.9080 - val_loss: 1.1921e-07 - val_acc: 0.9136
Epoch 5/200
1760/1760 [==============================] - 109s - loss: 1.1921e-07 - acc: 0.9080 - val_loss: 1.1921e-07 - val_acc: 0.9136
Epoch 6/200
1760/1760 [==============================] - 109s - loss: 1.1921e-07 - acc: 0.9080 - val_loss: 1.1921e-07 - val_acc: 0.9136
Epoch 7/200


Turns out that this is giving nan while predicting. Not sure why. convoutputs, etc were all 0s. or nans
on cpu it was giving loss as nan even while training. on gpu it was showing above output but predictions were null.

lr of .001 and .01 were stuck in loss of ~1.4

converging for lr=1e-5 with data trainsmaller. Will check on trainsmall.

================================================
Now lets try batch normmalization
