This version is amazing

In 109th iteration i got 0.07 and 0.06 for train and validation loss with both accuracy above 90%
Though the file in interest is weights.108 shows . Not sure why weight file is 1 less may be because of counting from 0.

Similar result for 121 iteration.


Key modifications
1. Not loading pretrained weights. Training from scratch
2. Tried to put Learning rate and other SGD param close to or  as per original vgg16

Note that this does not have fc layers of vgg16 (It was removed by origninal author)



================================

Restarting training because the entire data got deleted on docker :(


===============================

16/12/2016

Error while running command
Exception: Error when checking model input: expected zeropadding2d_input_1 to have 4 dimensions, but got array with shape (0, 1)

Solution:
Fixed by adding pos and neg folders and giving the correct path --dataset_path
 python cam.py --dataset_path "../../trainsmall/" --train TRAIN

Error
ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,256,56,56]
         [[Node: Conv2D_6 = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/gpu:1"](transpose_22, transpose_23)]]

Solution:
Switched to cpu mode for access to higher memory

-------rama + ../../trainsmall/pos ../../trainsmall/neg
Training..
Train on 1760 samples, validate on 440 samples
Epoch 1/200
1760/1760 [==============================] - 4839s - loss: 0.3273 - acc: 0.8949 - val_loss: 0.3064 - val_acc: 0.9068
Epoch 2/200
1760/1760 [==============================] - 4786s - loss: 0.2993 - acc: 0.9097 - val_loss: 0.2820 - val_acc: 0.9068
Epoch 3/200
1760/1760 [==============================] - 4834s - loss: 0.2703 - acc: 0.9097 - val_loss: 0.2426 - val_acc: 0.9068
Epoch 4/200
1760/1760 [==============================] - 4837s - loss: 0.2437 - acc: 0.9097 - val_loss: 0.2137 - val_acc: 0.9068
Epoch 5/200
1760/1760 [==============================] - 4838s - loss: 0.2629 - acc: 0.9097 - val_loss: 0.2222 - val_acc: 0.9068
Epoch 6/200
1760/1760 [==============================] - 4861s - loss: 0.2270 - acc: 0.9097 - val_loss: 0.2031 - val_acc: 0.9068
Epoch 7/200
1760/1760 [==============================] - 4526s - loss: 0.2981 - acc: 0.9097 - val_loss: 0.3001 - val_acc: 0.9068
Epoch 8/200
1760/1760 [==============================] - 4567s - loss: 0.2692 - acc: 0.9097 - val_loss: 0.2515 - val_acc: 0.9068
Epoch 9/200
1760/1760 [==============================] - 4525s - loss: 0.2584 - acc: 0.9097 - val_loss: 0.2643 - val_acc: 0.9068
Epoch 10/200
1760/1760 [==============================] - 4507s - loss: 0.2535 - acc: 0.9097 - val_loss: 0.2570 - val_acc: 0.9068
Epoch 11/200
1760/1760 [==============================] - 4584s - loss: 0.2480 - acc: 0.9097 - val_loss: 0.2446 - val_acc: 0.9068
Epoch 12/200
1760/1760 [==============================] - 4554s - loss: 0.2409 - acc: 0.9097 - val_loss: 0.2317 - val_acc: 0.9068
Epoch 13/200
1760/1760 [==============================] - 4488s - loss: 0.2377 - acc: 0.9097 - val_loss: 0.2248 - val_acc: 0.9068
Epoch 14/200
1760/1760 [==============================] - 4542s - loss: 0.2249 - acc: 0.9097 - val_loss: 0.2092 - val_acc: 0.9068
Epoch 15/200
1760/1760 [==============================] - 4574s - loss: 0.2266 - acc: 0.9097 - val_loss: 0.2171 - val_acc: 0.9068
Epoch 16/200
1760/1760 [==============================] - 4512s - loss: 0.2321 - acc: 0.9097 - val_loss: 0.3090 - val_acc: 0.9068
Epoch 17/200
  96/1760 [>.............................] - ETA: 3906s - loss: 0.1689 - acc: 0.9583

=========================================================== Got killed randomly. Do not know the reason

Running again by loading iteration 14 from the previous training. Not sure if the automatic validation split will take the
same files. But continuing as we have a different untouched training/validation data. Notice the slight improvement in all
parameters (suggests different validation set as these improvements are not seen in previous run if it continued.)

Training..
Train on 1760 samples, validate on 440 samples
Epoch 1/200
1760/1760 [==============================] - 2346s - loss: 0.2211 - acc: 0.9085 - val_loss: 0.2562 - val_acc: 0.9114
Epoch 2/200
1760/1760 [==============================] - 4708s - loss: 0.2363 - acc: 0.9085 - val_loss: 0.2524 - val_acc: 0.9114
Epoch 3/200
1760/1760 [==============================] - 4692s - loss: 0.2252 - acc: 0.9085 - val_loss: 0.2259 - val_acc: 0.9114
Epoch 4/200
1760/1760 [==============================] - 4701s - loss: 0.2202 - acc: 0.9085 - val_loss: 0.2232 - val_acc: 0.9114
Epoch 5/200
1760/1760 [==============================] - 4708s - loss: 0.2262 - acc: 0.9085 - val_loss: 0.2154 - val_acc: 0.9114
Epoch 6/200
1760/1760 [==============================] - 4695s - loss: 0.2137 - acc: 0.9085 - val_loss: 0.2320 - val_acc: 0.9114
Epoch 7/200
1760/1760 [==============================] - 4688s - loss: 0.2770 - acc: 0.9085 - val_loss: 0.2277 - val_acc: 0.9114
Epoch 8/200
1760/1760 [==============================] - 4706s - loss: 0.2089 - acc: 0.9085 - val_loss: 0.2233 - val_acc: 0.9114
Epoch 9/200
1760/1760 [==============================] - 4697s - loss: 0.2185 - acc: 0.9085 - val_loss: 0.2170 - val_acc: 0.9114
Epoch 10/200
1760/1760 [==============================] - 4698s - loss: 0.2073 - acc: 0.9085 - val_loss: 0.2143 - val_acc: 0.9114
Epoch 11/200
1760/1760 [==============================] - 4714s - loss: 0.2197 - acc: 0.9085 - val_loss: 0.2070 - val_acc: 0.9114
Epoch 12/200
1760/1760 [==============================] - 4734s - loss: 0.2095 - acc: 0.9085 - val_loss: 0.2224 - val_acc: 0.9114
Epoch 13/200
1760/1760 [==============================] - 4730s - loss: 0.2082 - acc: 0.9085 - val_loss: 0.2064 - val_acc: 0.9114
Epoch 14/200
1760/1760 [==============================] - 4753s - loss: 0.1972 - acc: 0.9085 - val_loss: 0.2160 - val_acc: 0.9114
Epoch 15/200
1760/1760 [==============================] - 4737s - loss: 0.1998 - acc: 0.9085 - val_loss: 0.2171 - val_acc: 0.9114
Epoch 16/200
1760/1760 [==============================] - 4753s - loss: 0.1975 - acc: 0.9097 - val_loss: 0.2029 - val_acc: 0.9114

===============] - 4737s - loss: 0.1998 - acc: 0.9085 - val_loss: 0.2171 - val_acc: 0.9114
Epoch 16/200
1760/1760 [==============================] - 4753s - loss: 0.1975 - acc: 0.9097 - val_loss: 0.2029 - val_acc: 0.9114
Epoch 17/200
1760/1760 [==============================] - 4763s - loss: 0.1884 - acc: 0.9085 - val_loss: 0.1940 - val_acc: 0.9114
Epoch 18/200
1760/1760 [==============================] - 4816s - loss: 0.1888 - acc: 0.9080 - val_loss: 0.1939 - val_acc: 0.9114
Epoch 19/200
1760/1760 [==============================] - 4800s - loss: 0.1860 - acc: 0.9057 - val_loss: 0.1903 - val_acc: 0.9114
Epoch 20/200
1760/1760 [==============================] - 4792s - loss: 0.2082 - acc: 0.9062 - val_loss: 0.2169 - val_acc: 0.9114
Epoch 21/200
1760/1760 [==============================] - 4774s - loss: 0.1969 - acc: 0.9085 - val_loss: 0.1903 - val_acc: 0.9114
Epoch 22/200
1760/1760 [==============================] - 4791s - loss: 0.1861 - acc: 0.9062 - val_loss: 0.2022 - val_acc: 0.9114
Epoch 23/200
1760/1760 [==============================] - 4790s - loss: 0.1883 - acc: 0.9057 - val_loss: 0.1920 - val_acc: 0.9114
Epoch 24/200
1760/1760 [==============================] - 4781s - loss: 0.1862 - acc: 0.9091 - val_loss: 0.1991 - val_acc: 0.9114
Epoch 25/200
1760/1760 [==============================] - 4804s - loss: 0.1873 - acc: 0.9074 - val_loss: 0.1942 - val_acc: 0.9114
Epoch 26/200
1760/1760 [==============================] - 4795s - loss: 0.1816 - acc: 0.9080 - val_loss: 0.2105 - val_acc: 0.9114
Epoch 27/200
1760/1760 [==============================] - 4810s - loss: 0.1814 - acc: 0.9074 - val_loss: 0.2424 - val_acc: 0.9114
Epoch 28/200
1760/1760 [==============================] - 4777s - loss: 0.1883 - acc: 0.9085 - val_loss: 0.1866 - val_acc: 0.9114
Epoch 29/200
1760/1760 [==============================] - 4827s - loss: 0.1850 - acc: 0.9085 - val_loss: 0.1972 - val_acc: 0.9091
Epoch 30/200
1760/1760 [==============================] - 4807s - loss: 0.1839 - acc: 0.9074 - val_loss: 0.1916 - val_acc: 0.9114
Epoch 31/200
1760/1760 [==============================] - 4816s - loss: 0.1780 - acc: 0.9085 - val_loss: 0.1852 - val_acc: 0.9114
Epoch 32/200
1760/1760 [==============================] - 4819s - loss: 0.1839 - acc: 0.9051 - val_loss: 0.1924 - val_acc: 0.9114
Epoch 33/200
1760/1760 [==============================] - 4857s - loss: 0.1815 - acc: 0.9085 - val_loss: 0.1888 - val_acc: 0.9159
Epoch 34/200
1760/1760 [==============================] - 4836s - loss: 0.1782 - acc: 0.9062 - val_loss: 0.1878 - val_acc: 0.9114
Epoch 35/200
1760/1760 [==============================] - 4872s - loss: 0.1767 - acc: 0.9068 - val_loss: 0.1867 - val_acc: 0.9114
Epoch 36/200
1760/1760 [==============================] - 4839s - loss: 0.1773 - acc: 0.9080 - val_loss: 0.1855 - val_acc: 0.9136
get_model()

Epoch 20/200
1760/1760 [==============================] - 4792s - loss: 0.2082 - acc: 0.9062 - val_loss: 0.2169 - val_acc: 0.9114
Epoch 21/200
1760/1760 [==============================] - 4774s - loss: 0.1969 - acc: 0.9085 - val_loss: 0.1903 - val_acc: 0.9114
Epoch 22/200
1760/1760 [==============================] - 4791s - loss: 0.1861 - acc: 0.9062 - val_loss: 0.2022 - val_acc: 0.9114
Epoch 23/200
1760/1760 [==============================] - 4790s - loss: 0.1883 - acc: 0.9057 - val_loss: 0.1920 - val_acc: 0.9114
Epoch 24/200
1760/1760 [==============================] - 4781s - loss: 0.1862 - acc: 0.9091 - val_loss: 0.1991 - val_acc: 0.9114
Epoch 25/200
1760/1760 [==============================] - 4804s - loss: 0.1873 - acc: 0.9074 - val_loss: 0.1942 - val_acc: 0.9114
Epoch 26/200
1760/1760 [==============================] - 4795s - loss: 0.1816 - acc: 0.9080 - val_loss: 0.2105 - val_acc: 0.9114
Epoch 27/200
1760/1760 [==============================] - 4810s - loss: 0.1814 - acc: 0.9074 - val_loss: 0.2424 - val_acc: 0.9114
Epoch 28/200
1760/1760 [==============================] - 4777s - loss: 0.1883 - acc: 0.9085 - val_loss: 0.1866 - val_acc: 0.9114
Epoch 29/200
1760/1760 [==============================] - 4827s - loss: 0.1850 - acc: 0.9085 - val_loss: 0.1972 - val_acc: 0.9091
Epoch 30/200
1760/1760 [==============================] - 4807s - loss: 0.1839 - acc: 0.9074 - val_loss: 0.1916 - val_acc: 0.9114
Epoch 31/200
1760/1760 [==============================] - 4816s - loss: 0.1780 - acc: 0.9085 - val_loss: 0.1852 - val_acc: 0.9114
Epoch 32/200
1760/1760 [==============================] - 4819s - loss: 0.1839 - acc: 0.9051 - val_loss: 0.1924 - val_acc: 0.9114
Epoch 33/200
1760/1760 [==============================] - 4857s - loss: 0.1815 - acc: 0.9085 - val_loss: 0.1888 - val_acc: 0.9159
Epoch 34/200
1760/1760 [==============================] - 4836s - loss: 0.1782 - acc: 0.9062 - val_loss: 0.1878 - val_acc: 0.9114
Epoch 35/200
1760/1760 [==============================] - 4872s - loss: 0.1767 - acc: 0.9068 - val_loss: 0.1867 - val_acc: 0.9114
Epoch 36/200
1760/1760 [==============================] - 4839s - loss: 0.1773 - acc: 0.9080 - val_loss: 0.1855 - val_acc: 0.9136
Epoch 37/200
1760/1760 [==============================] - 4858s - loss: 0.1790 - acc: 0.9080 - val_loss: 0.2012 - val_acc: 0.9205
Epoch 38/200
1760/1760 [==============================] - 4885s - loss: 0.1763 - acc: 0.9074 - val_loss: 0.1840 - val_acc: 0.9114
Epoch 39/200
1760/1760 [==============================] - 4892s - loss: 0.1763 - acc: 0.9085 - val_loss: 0.1950 - val_acc: 0.9114
Epoch 40/200
1760/1760 [==============================] - 4842s - loss: 0.1789 - acc: 0.9074 - val_loss: 0.1889 - val_acc: 0.9114
Epoch 41/200
1760/1760 [==============================] - 4870s - loss: 0.1765 - acc: 0.9085 - val_loss: 0.1877 - val_acc: 0.9136
Epoch 42/200
 640/1760 [=========>....................] - ETA: 2834s - loss: 0.1726 - acc: 0.9125

Killing it as i am not able to run quiver.


-------------------------------- How i made quiver to install successfully
git clone and then used the following comment from windows issue on github
It worked for me when I replaced 'quiverboard/dist/*' with ['quiverboard/dist/*'] in setup.py, compiled the client and then installed with python setup.py install.
----
